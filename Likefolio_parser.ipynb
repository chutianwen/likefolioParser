{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabula import read_pdf\n",
    "from tabulate import tabulate\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_prediction_dfs(path):\n",
    "    dfs = read_pdf(\n",
    "        path,\n",
    "        pages='all', \n",
    "        lattice=True,\n",
    "        pandas_options = {'header':None},\n",
    "        multiple_tables = True,\n",
    "        silent=True\n",
    "    )\n",
    "    \n",
    "    # usually data df has many columns, tunable. \n",
    "    valid_col_num = 7\n",
    "    valid_dfs = filter(lambda df: df.shape[-1] > valid_col_num, dfs)\n",
    "    raw_prediction_dfs = list(map(lambda df: df.dropna(how='all'), valid_dfs))\n",
    "    return raw_prediction_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prediction_dfs = get_raw_prediction_dfs('./data_es/LikeFolioSundayEarningsSheet20210313.pdf')\n",
    "raw_prediction_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_SEPARATOR = '\\r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def get_date(data):\n",
    "    fmts = ('%B %d %Y','%m/%d/%Y','%m/%d/%y','%b %Y','%B%Y','%b %d,%Y', \n",
    "            '%m-%d-%Y', '%Y-%m-%d')\n",
    "    for fmt in fmts:\n",
    "        try:\n",
    "            return dt.datetime.strptime(data, fmt)\n",
    "        except:\n",
    "            pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_date('2021-05-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine type of column\n",
    "This is scalable as new formats coming later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ColType(Enum):\n",
    "    MORE_LINE_TICKER = 1\n",
    "    THREE_LINE_TICKER = 2\n",
    "    NAN = 3\n",
    "    DATE = 4\n",
    "    OTHER = 5\n",
    "    \n",
    "def col_type(col):\n",
    "    if pd.isna(col):\n",
    "        return ColType.NAN\n",
    "    else:\n",
    "        values = col.split(LINE_SEPARATOR)        \n",
    "        if len(values) == 3 and values[1].isupper() and values[2].startswith('$'):\n",
    "            return ColType.THREE_LINE_TICKER\n",
    "        elif len(values) in [4, 5] and values[1].isupper() and values[2].startswith('$'):\n",
    "            # eg.'Wal-Mart\\rWMT\\r$148.23\\rTue before market'\n",
    "            return ColType.MORE_LINE_TICKER\n",
    "        elif len(values) == 2 and get_date(values[1]) :\n",
    "            # eg. 'Week 5\\r11/16/2020'\n",
    "            return ColType.DATE\n",
    "        else:\n",
    "            return ColType.OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols = [float('nan'), 'Kodak\\rKODK\\r$9.76\\rTue after market', 'Week 9\\r2021-03-15']\n",
    "for col in test_cols:\n",
    "    print(col_type(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_and_df_builder(raw_df):\n",
    "    # once seen this, following data should not be used, even for the following DF in \n",
    "    # the same file\n",
    "    stop_token = 'Unconfirmed Earnings'\n",
    "    is_unconfirmed = False\n",
    "    df_builder = []\n",
    "    week = ''\n",
    "    for idx, row in raw_df.iterrows():\n",
    "#         print(row.tolist())\n",
    "        if is_unconfirmed:\n",
    "            break\n",
    "        row_builder = []\n",
    "        cols = row.tolist()\n",
    "        # pointer to col\n",
    "        idx = 0\n",
    "        while idx < len(cols):\n",
    "            cur_col = cols[idx]\n",
    "            cur_type = col_type(cur_col)\n",
    "#             print(cur_type)\n",
    "\n",
    "            if cur_type == ColType.DATE:\n",
    "                week = get_date(cur_col.split(LINE_SEPARATOR)[-1])\n",
    "                break\n",
    "            elif cur_type == ColType.THREE_LINE_TICKER:\n",
    "                values = cur_col.split(LINE_SEPARATOR)\n",
    "                # Company, ticker\n",
    "                row_builder = [values[0], values[1]]\n",
    "                # Get weekday info \n",
    "                row_builder.append(cols[idx + 1])\n",
    "                # Get earning score\n",
    "                row_builder.append(cols[idx + 2])\n",
    "                df_builder.append(row_builder)\n",
    "                break\n",
    "            elif cur_type == ColType.MORE_LINE_TICKER:\n",
    "                values = cur_col.split(LINE_SEPARATOR)\n",
    "                # Company, ticker, weekday\n",
    "                row_builder = [values[0], values[1], values[3]]\n",
    "                # Get earning score\n",
    "                row_builder.append(cols[idx + 1])\n",
    "                df_builder.append(row_builder)\n",
    "                break\n",
    "            elif stop_token in str(cur_col):\n",
    "                is_unconfirmed = True\n",
    "                break\n",
    "            idx += 1\n",
    "        \n",
    "    \n",
    "    return week, df_builder, is_unconfirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earning_prediction_df_from_file(path):\n",
    "    raw_prediction_dfs = get_raw_prediction_dfs(path)\n",
    "    week_value = None \n",
    "    final_df_builder = []\n",
    "    for df in raw_prediction_dfs:\n",
    "        week, df_builder, is_following_unconfirmed = get_week_and_df_builder(df)\n",
    "        if week:\n",
    "            week_value = week\n",
    "        final_df_builder.extend(df_builder)\n",
    "        if is_following_unconfirmed:\n",
    "            break\n",
    "    final_df = pd.DataFrame(final_df_builder, columns = ['Company', 'Ticker', 'Earning Time', 'Earning Score'])\n",
    "    final_df['Starting Week'] = week_value\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = earning_prediction_df_from_file('./data_es\\LikeFolioSundayEarningsSheet_20201115.pdf')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate the whole folder and merge data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data_es'\n",
    "PATHS = [join(DATA_FOLDER, f) for f in listdir(DATA_FOLDER) if isfile(join(DATA_FOLDER, f))]\n",
    "earning_prediction_dfs = []\n",
    "merged_df = None\n",
    "success_counter = 0\n",
    "for path in PATHS:\n",
    "    print(f'Processing {path} ...')\n",
    "    try:\n",
    "        earning_prediction_df = earning_prediction_df_from_file(path)\n",
    "        print(earning_prediction_df.shape)\n",
    "        if earning_prediction_df.shape[0] > 0: \n",
    "            earning_prediction_dfs.append(earning_prediction_df)\n",
    "            success_counter += 1\n",
    "        else:\n",
    "            print(f'[Warning]: Empty dataframe extracted {path}')\n",
    "    except:\n",
    "        print(f'Cannot process {path}')\n",
    "if len(earning_prediction_dfs) > 0:\n",
    "    merged_df = pd.concat(earning_prediction_dfs)\n",
    "print(f'Processed {success_counter} files out of {len(PATHS)} raw files')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further filter to remove data of unconfirmed date\n",
    "print(f'Before trimming there are {merged_df.shape[0]} rows')\n",
    "df_trim_unconfirmed = merged_df[merged_df['Earning Time'].apply(len) > 5] \n",
    "final_df = df_trim_unconfirmed.drop_duplicates()\n",
    "print(f'After trimming there are {final_df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "def er_date(starting_week_col, weekday_col):\n",
    "    weekday = weekday_col.split(' ')[0]\n",
    "    offset_map = {\n",
    "        'Mon': 0,\n",
    "        'Tue': 1,\n",
    "        'Wed': 2,\n",
    "        'Thu': 3,\n",
    "        'Fri': 4\n",
    "    }\n",
    "    if weekday in offset_map:\n",
    "        er_dt = starting_week_col + datetime.timedelta(offset_map[weekday])\n",
    "        return er_dt\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_str = '2018-06-29'\n",
    "dt = datetime.datetime.strptime(date_time_str, '%Y-%m-%d')\n",
    "er_date(dt, 'Mon asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Earning Date'] = final_df.apply(lambda x: er_date(x['Starting Week'], x['Earning Time']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREMARKET = 'PREMARKET'\n",
    "AFH = 'AFH'\n",
    "\n",
    "def market_time(earning_time_col):\n",
    "    if 'before' in str(earning_time_col):\n",
    "        return PREMARKET\n",
    "    if 'after' in str(earning_time_col):\n",
    "        return AFH\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Market Time'] = final_df.apply(lambda x: market_time(x['Earning Time']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('earning_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use yahoo API to get stock quotes\n",
    "\n",
    "There's a parallel query version script called 'parallel_query_yahoo.py', use this for large data processing. You need use python to run rather than from notebook here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf  \n",
    "\n",
    "def get_quotes(row):\n",
    "    ticker, earning_dt, market_time = row['Ticker'], row['Earning Date'], row['Market Time']\n",
    "    if earning_dt and market_time:\n",
    "        # actual is end_dt - 1, need +1 to offset API requirment\n",
    "        if market_time == PREMARKET:\n",
    "            start_dt = earning_dt + datetime.timedelta(-1)\n",
    "            end_dt = earning_dt + datetime.timedelta(1)\n",
    "        elif market_time == AFH:\n",
    "            start_dt = earning_dt\n",
    "            end_dt = earning_dt + datetime.timedelta(2)\n",
    "        else:\n",
    "            raise \n",
    "        start_dt = str(start_dt).split(' ')[0]\n",
    "        end_dt = str(end_dt).split(' ')[0]\n",
    "        print(start_dt, end_dt)\n",
    "    \n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            quotes = stock.history(start=start_dt, end=end_dt).round(2)\n",
    "            start_quotes = quotes.iloc[0]\n",
    "            end_quotes = quotes.iloc[1]\n",
    "            row['Left-day Open'] = start_quotes.Open\n",
    "            row['Right-day Open'] = end_quotes.Open\n",
    "            row['Left-day Close'] = start_quotes.Close\n",
    "            row['Right-day Close'] = end_quotes.Close\n",
    "            row['Left-day High'] = start_quotes.High\n",
    "            row['Right-day High'] = end_quotes.High\n",
    "            row['Left-day Low'] = start_quotes.Low\n",
    "            row['Right-day Low'] = end_quotes.Low\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
